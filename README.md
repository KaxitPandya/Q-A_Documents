# ğŸ“š Document Q&A Assistant â€” Advanced Adaptive RAG Pipeline

> **Live demo:** [q-adocuments.streamlit.app](https://q-adocuments.streamlit.app/)

An AI-powered **Retrieval-Augmented Generation (RAG)** system with an **Adaptive Router** that auto-selects the optimal pipeline depth per query â€” featuring Self-RAG, streaming answers, conversation summarization, semantic chunking, hybrid search, LLM re-ranking, Corrective RAG, hallucination detection, and autonomous agentic reasoning.

Built with **LangChain LCEL** Â· **OpenAI** Â· **ChromaDB** Â· **Streamlit**

---

## ğŸ§  What Makes This Different

Most RAG apps use a static pipeline: embed â†’ vector search â†’ generate. This project implements **Adaptive RAG** â€” the system *intelligently routes* each query through the optimal pipeline:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ§­ ADAPTIVE RAG ROUTER                        â”‚
â”‚  LLM classifies query complexity â†’ selects pipeline depth        â”‚
â”‚                                                                   â”‚
â”‚  "hello"            â†’ no_retrieval  (Self-RAG: skip retrieval)   â”‚
â”‚  "What is revenue?" â†’ simple        (hybrid search only)         â”‚
â”‚  "Summarize risks"  â†’ moderate      (hybrid + CRAG)              â”‚
â”‚  "Compare Ch2 & 5"  â†’ complex       (full pipeline + grounding)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. SELF-RAG CHECK                                                â”‚
â”‚     Does this query even need retrieval?                          â”‚
â”‚     Greetings, math, meta-questions â†’ answer directly             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  2. QUESTION REFORMULATION                                        â”‚
â”‚     Rewrite follow-ups ("what about its CEO?") into               â”‚
â”‚     self-contained queries for better retrieval                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  3. SMART CHUNKING  (3 strategies)                                â”‚
â”‚     â€¢ Recursive (fast, character-based)                           â”‚
â”‚     â€¢ Semantic  (embedding-based boundary detection)              â”‚
â”‚     â€¢ Parent-Child (small chunks for retrieval,                   â”‚
â”‚       large parents for context)                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  4. HYBRID SEARCH  (BM25 + Vector â†’ RRF)                         â”‚
â”‚     Keyword retrieval (BM25 Okapi) + semantic retrieval           â”‚
â”‚     fused with Reciprocal Rank Fusion                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  5. LLM RE-RANKING                                                â”‚
â”‚     LLM scores each chunk 0-10 for relevance                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  6. CORRECTIVE RAG (CRAG)                                         â”‚
â”‚     Grade retrieval: good / partial / poor                        â”‚
â”‚     If poor â†’ automatic Wikipedia fallback                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  7. STREAMING ANSWER GENERATION                                   â”‚
â”‚     Token-by-token streaming via LCEL chain.stream()              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  8. HALLUCINATION GROUNDING CHECK                                 â”‚
â”‚     Post-generation: is the answer supported by context?          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  9. CONVERSATION MEMORY                                           â”‚
â”‚     Progressive summarization â€” older messages are LLM-summarized â”‚
â”‚     instead of truncated, preserving long-range context           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Three Pipeline Modes

| Mode | Description |
|------|-------------|
| **ğŸ§  Adaptive** (default) | Auto-classifies each query and selects the right pipeline depth. Simple â†’ fast. Complex â†’ full pipeline. |
| **ğŸ¤– Agent** | Autonomous tool-calling agent (OpenAI function calling) that decides whether to search documents, search Wikipedia, or answer directly. |
| **ğŸ”§ Manual** | Full manual control over every pipeline stage for power users. |

---

## âœ¨ Feature List

| Category | Feature | Description |
|----------|---------|-------------|
| **Router** | Adaptive RAG | LLM classifies query â†’ auto-selects pipeline depth |
| **Router** | Self-RAG | Skips retrieval entirely for greetings, math, meta-questions |
| **Chunking** | Semantic chunking | Splits at *meaning* boundaries using embedding cosine similarity |
| **Chunking** | Parent-child | Small chunks for retrieval, large parents for LLM context |
| **Retrieval** | Hybrid search | BM25 keyword + vector semantic, fused via RRF |
| **Retrieval** | LLM re-ranking | LLM scores each chunk's relevance 0-10 |
| **Retrieval** | Query expansion | Multi-query: LLM rephrases question 2 ways for broader recall |
| **Quality** | Corrective RAG | Grades retrieval, falls back to Wikipedia if poor |
| **Quality** | Grounding check | Detects hallucination: is answer supported by context? |
| **Quality** | Confidence scoring | Multi-signal: word overlap + CRAG grade + grounding |
| **UX** | Streaming answers | Token-by-token display via LCEL chain.stream() |
| **UX** | Conversation memory | Progressive LLM summarization of older messages |
| **UX** | Pipeline visibility | Expandable "pipeline steps" shows exactly what ran |
| **UX** | Route badges | Visual indicator of which route was selected per query |
| **Agent** | Tool-calling agent | Autonomous reasoning: documents, Wikipedia, direct answer |
| **Infra** | Retry with backoff | Exponential retry for all OpenAI calls |
| **Infra** | Token & cost tracking | Live tokens + USD estimate in sidebar |

---

## ğŸ—ï¸ Architecture

```
app.py                 â† Streamlit UI & orchestration
â”‚
â”œâ”€â”€ router.py          â† Adaptive RAG Router + Self-RAG classifier
â”œâ”€â”€ memory.py          â† Conversation summarization (progressive)
â”œâ”€â”€ chunking.py        â† SemanticChunker, ParentChildChunker, recursive
â”œâ”€â”€ retrieval.py       â† BM25, RRF fusion, re-ranking, CRAG, grounding
â”œâ”€â”€ qa_chain.py        â† RAG pipeline (prepare_context + stream/invoke)
â”œâ”€â”€ agent.py           â† Tool-calling RAG agent (OpenAI functions)
â”œâ”€â”€ vector_store.py    â† ChromaDB embedding with retry
â”œâ”€â”€ document_loader.py â† File loaders (PDF, DOCX, TXT, MD, CSV, Wikipedia)
â”œâ”€â”€ config.py          â† All settings, model configs
â””â”€â”€ logger.py          â† Structured logging
```

### Key Design Decisions

- **Adaptive routing** replaces manual presets â€” the system is smarter about cost/latency/quality tradeoffs than the user
- **Streaming** splits the pipeline into `prepare_rag_context()` (blocking retrieval) + `generate_answer_stream()` (streaming generation)
- **Progressive summarization** uses the LLM to compress older conversations instead of hard-truncating at N messages
- **Self-RAG** (no-retrieval route) saves latency and cost for questions that don't need document lookup

---

## ğŸš€ Quick Start

### Local development

```bash
git clone https://github.com/<your-username>/Q-A_Documents.git
cd Q-A_Documents

python -m venv .venv
.venv\Scripts\activate        # Windows
# source .venv/bin/activate   # macOS/Linux

pip install -r requirements.txt
streamlit run app.py
```

### Streamlit Cloud (production)

The app is deployed at [q-adocuments.streamlit.app](https://q-adocuments.streamlit.app/).
API key is configured via Streamlit Secrets (`openai_api_key`).

---

## ğŸ§° Tech Stack

| Layer | Technology |
|-------|-----------|
| LLM | OpenAI GPT-4o-mini / GPT-4o |
| Embeddings | text-embedding-3-small |
| Vector DB | ChromaDB |
| Keyword search | BM25 Okapi (rank_bm25) |
| Framework | LangChain LCEL + Agents |
| Frontend | Streamlit |
| Language | Python 3.10+ |

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ app.py               # Streamlit UI â€” main entry point
â”œâ”€â”€ router.py            # Adaptive RAG Router + Self-RAG
â”œâ”€â”€ memory.py            # Conversation summarization
â”œâ”€â”€ chunking.py          # Semantic, parent-child, recursive chunking
â”œâ”€â”€ retrieval.py         # Hybrid search, RRF, re-ranking, CRAG, grounding
â”œâ”€â”€ qa_chain.py          # RAG pipeline (context prep + streaming)
â”œâ”€â”€ agent.py             # Agentic RAG with tool calling
â”œâ”€â”€ vector_store.py      # ChromaDB embedding & retry
â”œâ”€â”€ document_loader.py   # File loading & preprocessing
â”œâ”€â”€ config.py            # Configuration & constants
â”œâ”€â”€ logger.py            # Structured logging
â”œâ”€â”€ requirements.txt     # Dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ“š Research References

This project implements concepts from several RAG research papers:

| Paper | Concept Used |
|-------|-------------|
| [Adaptive RAG (Jeong et al., 2024)](https://arxiv.org/abs/2403.14403) | Query complexity classification â†’ adaptive pipeline |
| [Self-RAG (Asai et al., 2023)](https://arxiv.org/abs/2310.11511) | Decide when retrieval is needed |
| [Corrective RAG (Yan et al., 2024)](https://arxiv.org/abs/2401.15884) | Grade retrieval â†’ corrective action |
| [Reciprocal Rank Fusion (Cormack et al., 2009)](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) | Fuse multiple ranked lists |

---

## ğŸ“ˆ Future Enhancements

- [ ] RAGAS / DeepEval automated evaluation pipeline
- [ ] Open-source LLM support (Ollama) for local inference
- [ ] Graph RAG for relationship-heavy documents
- [ ] Multi-modal: image/table extraction from PDFs
- [ ] Excel / PowerPoint file support

---

## ğŸ“„ License

MIT
